# @package _global_

# to execute this experiment run:
# python src/train.py experiment=ddpm_dev

defaults:
  - override /data: lidc_2d
  - override /model: ddpm
  - override /callbacks: diffusion
  - override /trainer: default
  - override /logger: wandb

tags: ["ddpm", "diffusion", "lidc"]
task_name: "train_diffusion"

data:
  batch_size: 32
  include_clean: False
  train_val_test_split: [6, 2, 2]

model:
  net:
    _target_: src.models.components.diffusion.unet2d.UNetModel
    image_size: 16
    in_channels: 16
    out_channels: 16
    model_channels: 192
    channel_mult: [1, 2, 4]
    attention_resolutions: [1, 2, 4, 8]
    num_heads: 8
    num_res_blocks: 2
  autoencoder_ckpt_path: "/work/hpc/pgl/lung-diffusion/outputs/vq_gan_2d_seg2/checkpoints/epoch_080.ckpt"

logger: 
  wandb:
    name: "cnn_attn_ddpm_2d"
    id: "scngqepk"
  
callbacks:
  model_checkpoint:
    monitor: "val/loss"
    mode: "min"
    save_last: True
    save_on_train_epoch_end: True
  log_metrics:
    fid:
      _target_: torchmetrics.image.FrechetInceptionDistance
      feature: 2048
      normalize: True
trainer:
  max_epochs: 1000

ckpt_path: "/work/hpc/pgl/lung-diffusion/logs/train_diffusion/runs/2024-05-09_18-27-14/lung-thesis/scngqepk/checkpoints/epoch=63-step=22208.ckpt"
